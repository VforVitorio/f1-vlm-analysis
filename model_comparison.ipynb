{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VLM Model Comparison for Caption Generation\n",
    "\n",
    "This notebook compares evaluation metrics of 3 Vision-Language Models (VLM) for caption generation on the F1 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models to compare\n",
    "models = ['blip', 'git-base', 'swin-tiny']\n",
    "\n",
    "# Load metrics for each model\n",
    "metrics_data = {}\n",
    "for model in models:\n",
    "    with open(f'results/{model}/metrics.json', 'r') as f:\n",
    "        metrics_data[model] = json.load(f)\n",
    "\n",
    "print(\"Metrics loaded for:\", list(metrics_data.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Comparison Table\n",
    "\n",
    "### Available Metrics:\n",
    "\n",
    "- **BLEU-1 to BLEU-4**: Measure n-gram precision (individual words, pairs, triplets, quadruplets). Higher values indicate better similarity with ground truth.\n",
    "- **METEOR**: Metric that considers synonyms and stemming, more robust than BLEU. Range [0, 1].\n",
    "- **ROUGE-L**: Measures the longest common subsequence. Useful for evaluating structural coherence of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame for comparison\n",
    "df = pd.DataFrame(metrics_data).T\n",
    "df.index.name = 'Model'\n",
    "\n",
    "# Display table\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"METRICS COMPARISON\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "print(df.round(4))\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Identify best model per metric\n",
    "print(\"\\nBEST MODEL PER METRIC:\")\n",
    "print(\"-\"*70)\n",
    "for metric in df.columns:\n",
    "    best_model = df[metric].idxmax()\n",
    "    best_score = df[metric].max()\n",
    "    print(f\"{metric:<12} â†’ {best_model.upper():<12} ({best_score:.4f})\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Comparative Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouped bar chart\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Subplot 1: BLEU scores\n",
    "bleu_cols = [col for col in df.columns if 'BLEU' in col]\n",
    "df[bleu_cols].plot(kind='bar', ax=axes[0], width=0.8)\n",
    "axes[0].set_title('BLEU Scores (N-gram Precision)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Model', fontsize=11)\n",
    "axes[0].set_ylabel('Score', fontsize=11)\n",
    "axes[0].legend(title='Metric', fontsize=9)\n",
    "axes[0].set_xticklabels(df.index, rotation=0)\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Subplot 2: METEOR and ROUGE-L\n",
    "other_cols = ['METEOR', 'ROUGE-L']\n",
    "df[other_cols].plot(kind='bar', ax=axes[1], width=0.7, color=['#2ecc71', '#e74c3c'])\n",
    "axes[1].set_title('METEOR & ROUGE-L Scores', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Model', fontsize=11)\n",
    "axes[1].set_ylabel('Score', fontsize=11)\n",
    "axes[1].legend(title='Metric', fontsize=9)\n",
    "axes[1].set_xticklabels(df.index, rotation=0)\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nChart saved to: results/model_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Summary Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average of all metrics (normalized)\n",
    "df_normalized = df.copy()\n",
    "for col in df_normalized.columns:\n",
    "    max_val = df_normalized[col].max()\n",
    "    if max_val > 0:\n",
    "        df_normalized[col] = df_normalized[col] / max_val\n",
    "\n",
    "df_normalized['Average'] = df_normalized.mean(axis=1)\n",
    "\n",
    "print(\"\\nAVERAGE PERFORMANCE (normalized):\")\n",
    "print(\"-\"*40)\n",
    "for model in df_normalized.index:\n",
    "    avg_score = df_normalized.loc[model, 'Average']\n",
    "    print(f\"{model.upper():<12}: {avg_score:.4f} ({avg_score*100:.1f}%)\")\n",
    "\n",
    "best_overall = df_normalized['Average'].idxmax()\n",
    "print(f\"\\nBest overall model: {best_overall.upper()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
